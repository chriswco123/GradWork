#create AWS EMR hadoop cluster with x4.large, 2 number of instances, and select an already exist EC2 key pair.
#in git bash, ssh to the cluster using the Master Public DNS.
#check for java version
java -version

#create group and user
sudo groupadd hadoop
sudo adduser --ingroup hadoop hduser

#download and extract hadoop-0.20.203.0rc1 from Apache archives, which is the default version assumed in Giraph
su - hdadmin
cd /usr/local
sudo wget http://archive.apache.org/dist/hadoop/core/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz
sudo tar xzf hadoop-0.20.203.0rc1.tar.gz
sudo mv hadoop-0.20.203.0 hadoop
sudo chown -R hduser:hadoop hadoop

#swich to user account hduser and edit the account's $HOME/.bashrc with
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

#edit $HADOOP_HOME/conf/hadoop-env.sh with
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

#create a base temporary directorty for local FS and HDFS files
su - hdadmin
sudo mkdir -p /app/hadoop/tmp
sudo chown hduser:hadoop /app/hadoop/tmp
sudo chmod 750 /app/hadoop/tmp

#Edit core-site.xml with
<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
</property>

<property> 
<name>fs.default.name</name> 
<value>hdfs://hdnode01:54310</value> 
</property>

#Edit mapred-site.xml with
<property>
<name>mapred.job.tracker</name> 
<value>hdnode01:54311</value>
</property>

<property>
<name>mapred.tasktracker.map.tasks.maximum</name>
<value>4</value>
</property>

<property>
<name>mapred.map.tasks</name>
<value>4</value>
</property>

#Edit hdfs-site.xml with
<property>
<name>dfs.replication</name> 
<value>1</value> 
</property>

#SSH to hdnode01 under user account hduser
#initialize HDFS
#start the HDFS and the map/reduce
$HADOOP_HOME/bin/hadoop namenode -format
$HADOOP_HOME/bin/start-dfs.sh
$HADOOP_HOME/bin/start-mapred.sh

#Running the map/reduce job
cd /tmp/
wget http://www.gutenberg.org/cache/epub/132/pg132.txt
$HADOOP_HOME/bin/hadoop dfs -copyFromLocal /tmp/pg132.txt /user/hduser/input/pg132.txt
$HADOOP_HOME/bin/hadoop dfs -ls /user/hduser/input

#run wordcount
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-examples-0.20.203.0.jar wordcount /user/hduser/input/pg132.txt /user/hduser/output/wordcount

#check the output
$HADOOP_HOME/bin/hadoop dfs -cat /user/hduser/output/wordcount/p* | less
